debug: false


project: ml-skeleton
name: default

notes: null


batch_size: 16


model:

    type: reference_llama.modelling_llama.LlamaForCausalLM
    pretrained: null

    config:

        type: reference_llama.configuration_llama.LlamaConfig

        kwargs:

            bos_token_id: 1
            eos_token_id: 2

            hidden_act: silu
            hidden_size: 1024

            initializer_range: 0.02
            intermediate_size: 2048
            max_position_embeddings: 2048
            
            num_attention_heads: 16
            num_hidden_layers: 24
            num_key_value_heads: 4

            rms_norm_eps: 1e-05
            rope_scaling: null
            tie_word_embeddings: false

            vocab_size: 32001

            _attn_implementation: flash_attention_2

trainer:

    type: lm_trainer.LMTrainer

    gradient_checkpointing: true

    autocast_dtype: bfloat16

    checkpoint_interval: 1000




optimizer:

    type: adamw.AdamW

    kwargs:

        lr: 0.0001
        weight_decay: 0.01


dataset:

    name: HuggingFaceFW/fineweb-edu

    kwargs:

        split: train

        streaming: true


collator:

    type: simple_tokenizer.SimpleTokenizer

    kwargs:

        tokenizer_url: TinyLlama/TinyLlama_v1.1
        max_length: 256
