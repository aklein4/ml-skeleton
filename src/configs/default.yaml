debug: false


project: ml-skeleton
name: default

notes: null

seed: 42

batch_size: 16


model:

    type: reference_llama.modelling_llama.LlamaForCausalLM
    pretrained: null

    config:

        type: reference_llama.configuration_llama.LlamaConfig

        kwargs:

            bos_token_id: 1
            eos_token_id: 2

            hidden_act: silu
            hidden_size: 1024

            initializer_range: 0.02
            intermediate_size: 2048
            max_position_embeddings: 2048
            
            num_attention_heads: 16
            num_hidden_layers: 24
            num_key_value_heads: 4

            rms_norm_eps: 1e-05
            rope_scaling: null
            tie_word_embeddings: false

            vocab_size: 32001

            _attn_implementation: flash_attention_2

trainer:

    type: lm_trainer.LMTrainer

    gradient_checkpointing: true
    autocast_dtype: bfloat16
    grad_norm_clip: 1.0

    checkpoint_interval: 1000
    manual_checkpoint_interval: 10
    push_to_hub: true


optimizer:

    type: adamw.AdamW

    kwargs:

        lr: 2e-4
        final_lr: 2e-5

        num_warmup_steps: 500
        num_training_steps: 1000000

        betas: [0.9, 0.95]
        eps: 1e-8

        weight_decay: 0.1

        grad_clip: null

        nan_safe: false


dataset:

    name: HuggingFaceFW/fineweb-edu

    kwargs:

        split: train

        streaming: true


collator:

    type: simple_tokenizer.SimpleTokenizer

    kwargs:

        tokenizer_url: TinyLlama/TinyLlama_v1.1
        max_length: 256
